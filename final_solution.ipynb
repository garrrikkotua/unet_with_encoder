{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Setting paths"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread\nimport cv2\n\nfrom tqdm import tqdm\nfrom skimage.util import montage\nfrom skimage.morphology import label\nimport logging\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom tqdm import tqdm\n\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader, random_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"../input/almaz-antey-hackathon-l1\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['train', 'train_segmentation.csv', 'test', 'sample_submission.csv']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install segmentation-models-pytorch","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting segmentation-models-pytorch\n  Downloading segmentation_models_pytorch-0.1.2-py3-none-any.whl (53 kB)\n\u001b[K     |████████████████████████████████| 53 kB 529 kB/s eta 0:00:011\n\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\nCollecting pretrainedmodels==0.7.4\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[K     |████████████████████████████████| 58 kB 1.8 MB/s eta 0:00:011\n\u001b[?25hCollecting timm==0.1.20\n  Downloading timm-0.1.20-py3-none-any.whl (161 kB)\n\u001b[K     |████████████████████████████████| 161 kB 3.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: torchvision>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch) (0.7.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.6.0)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.45.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.18.5)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (8.0.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (0.18.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.14.0)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12419 sha256=64c316605f11e1df3d05ecbed52bde1bb5830021434780e0e728102b4e892c9e\n  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60962 sha256=4200974d58fdf3a5cef30e947edc85609633350c6339246fad8b5029dd3af75d\n  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: efficientnet-pytorch, pretrainedmodels, timm, segmentation-models-pytorch\nSuccessfully installed efficientnet-pytorch-0.6.3 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.1.2 timm-0.1.20\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install albumentations","execution_count":3,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: albumentations in /opt/conda/lib/python3.7/site-packages (0.5.0)\nRequirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (4.4.0.44)\nRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations) (5.3.1)\nRequirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.16.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.4.1)\nRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.18.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.14.0)\nRequirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (8.0.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (3.2.1)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (4.4.0.44)\nRequirement already satisfied: imageio in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (2.8.0)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.4)\nRequirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = os.listdir('../input/almaz-antey-hackathon-l1/train/train')\nprint(len(train))\n\ntest = os.listdir('../input/almaz-antey-hackathon-l1/test/test')\nprint(len(test))","execution_count":4,"outputs":[{"output_type":"stream","text":"16343\n2957\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"ship_dir = '../input/almaz-antey-hackathon-l1/'\ntrain_image_dir = os.path.join(ship_dir, 'train/train')\ntest_image_dir = os.path.join(ship_dir, 'test/test')","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load csv-data files\ntrain_df = pd.read_csv(os.path.join(ship_dir, 'train_segmentation.csv'))\nsample_sub = pd.read_csv(os.path.join(ship_dir, 'sample_submission.csv'))","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"   Unnamed: 0        ImageId  \\\n0       12166  000155de5.jpg   \n1        2341  000194a2d.jpg   \n2        1981  00021ddc3.jpg   \n3           0  0005d6d95.jpg   \n4        5703  0017c19d6.jpg   \n\n                                       EncodedPixels  \n0  264661 17 265429 33 266197 33 266965 33 267733...  \n1  51834 9 52602 9 53370 9 54138 9 54906 9 55674 ...  \n2  74441 7 75207 9 75972 12 76738 14 77506 10 775...  \n3  265143 1 265910 4 266678 5 267445 7 268212 10 ...  \n4  329228 1 329995 3 330762 4 331529 6 332296 8 3...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>ImageId</th>\n      <th>EncodedPixels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12166</td>\n      <td>000155de5.jpg</td>\n      <td>264661 17 265429 33 266197 33 266965 33 267733...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2341</td>\n      <td>000194a2d.jpg</td>\n      <td>51834 9 52602 9 53370 9 54138 9 54906 9 55674 ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1981</td>\n      <td>00021ddc3.jpg</td>\n      <td>74441 7 75207 9 75972 12 76738 14 77506 10 775...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0005d6d95.jpg</td>\n      <td>265143 1 265910 4 266678 5 267445 7 268212 10 ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5703</td>\n      <td>0017c19d6.jpg</td>\n      <td>329228 1 329995 3 330762 4 331529 6 332296 8 3...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Run Length Decoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"montage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\n\ndef multi_rle_encode(img, **kwargs):\n    '''\n    Encode connected regions as separated masks\n    '''\n    labels = label(img)\n    if img.ndim > 2:\n        return [rle_encode(np.sum(labels==k, axis=2), **kwargs) for k in np.unique(labels[labels>0])]\n    else:\n        return [rle_encode(labels==k, **kwargs) for k in np.unique(labels[labels>0])]\n\n# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\ndef rle_encode(img, min_max_threshold=1e-4, max_mean_threshold=None):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    if np.max(img) < min_max_threshold:\n        return '' ## no need to encode if it's all zeros\n    if max_mean_threshold and np.mean(img) > max_mean_threshold:\n        return '' ## ignore overfilled mask\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle_decode(mask_rle, shape=(768, 768)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T  # Needed to align to RLE direction\n\ndef masks_as_image(in_mask_list, **kwargs):\n    # Take the individual ship masks and create a single mask array for all ships\n    all_masks = np.zeros(kwargs['shape'], dtype = np.uint8)\n    for mask in in_mask_list:\n        if isinstance(mask, str):\n            all_masks |= rle_decode(mask, **kwargs)\n    return all_masks\n\ndef masks_as_color(in_mask_list, **kwargs):\n    # Take the individual ship masks and create a color mask array for each ships\n    all_masks = np.zeros(kwargs['shape'], dtype = np.float)\n    scale = lambda x: (len(in_mask_list) + x + 1) / (len(in_mask_list) * 2) ## scale the heatmap image to shift \n    for i,mask in enumerate(in_mask_list):\n        if isinstance(mask, str):\n            all_masks[:,:] += scale(i) * rle_decode(mask, **kwargs)\n    return all_masks","execution_count":64,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple data generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ship_generator(database, image_path, batch_size=9):\n    all_batches = list(database.groupby('ImageId'))\n    out_rgb = []\n    out_masks = []\n    while True:\n        np.random.shuffle(all_batches)  # shuffle\n        for c_img_id, c_masks in all_batches:\n            rgb_path = os.path.join(image_path, c_img_id)\n            c_img = imread(rgb_path)\n            c_mask = np.expand_dims(masks_as_color(c_masks['EncodedPixels'].values, shape=(768, 768)), -1)\n                \n            out_rgb += [c_img]\n            out_masks += [c_mask]\n            if len(out_rgb) >= batch_size:\n                yield np.stack(out_rgb, 0)/255.0, np.stack(out_masks, 0)\n                out_rgb = []\n                out_masks = []","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Torch Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom os.path import splitext\nfrom os import listdir\nimport numpy as np\nfrom glob import glob\nimport torch\nfrom torch.utils.data import Dataset\nimport logging\nfrom PIL import Image","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BasicDataset(Dataset):\n    def __init__(self, database, image_path, preprocessing=None):\n        self.database = database\n        self.image_path = image_path\n        self.all_batches = list(database.groupby('ImageId'))\n        self.preprocessing = preprocessing\n\n    def __len__(self):\n        return len(self.all_batches)\n    \n    \n    def __getitem__(self, idx):\n        c_img_id, c_masks = self.all_batches[idx]\n        rgb_path = os.path.join(self.image_path, c_img_id)\n        c_img = imread(rgb_path)\n        c_mask = np.expand_dims(masks_as_color(c_masks['EncodedPixels'].values, shape=(768, 768)), -1)\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=c_img, mask=c_mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import segmentation_models_pytorch as smp\nimport albumentations as albu","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\n\ndef get_preprocessing(preprocessing_fn):\n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n    return albu.Compose(_transform)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ENCODER = 'se_resnext50_32x4d'\nENCODER_WEIGHTS = 'imagenet'\nCLASSES = ['ship']\nACTIVATION = 'sigmoid'\nDEVICE = 'cuda'\n\nmodel = smp.Unet(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES), \n    activation=ACTIVATION,\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","execution_count":14,"outputs":[{"output_type":"stream","text":"Downloading: \"http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth\" to /root/.cache/torch/hub/checkpoints/se_resnext50_32x4d-a260b3a4.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=110559176.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60388a76d0214b11ac2b004d82fa6371"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_percent = 0.1\n\ndataset = BasicDataset(train_df, train_image_dir, get_preprocessing(preprocessing_fn))\nn_val = int(len(dataset) * val_percent)\nn_train = len(dataset) - n_val\ntrain, val = random_split(dataset, [n_train, n_val])\ntrain_loader = DataLoader(train, batch_size=4, shuffle=True, num_workers=12)\nvalid_loader = DataLoader(val, batch_size=1, shuffle=False, num_workers=4)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = smp.utils.losses.DiceLoss()\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n]\n\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0001),\n])","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_score = 0\n\nfor i in range(0, 5):\n    \n    print('\\nEpoch: {}'.format(i))\n    train_logs = train_epoch.run(train_loader)\n    valid_logs = valid_epoch.run(valid_loader)\n    \n    # do something (save model, change lr, etc.)\n    if max_score < valid_logs['iou_score']:\n        max_score = valid_logs['iou_score']\n        torch.save(model, 'best_model.pth')\n        print('Model saved!')","execution_count":25,"outputs":[{"output_type":"stream","text":"\nEpoch: 0\ntrain:  72%|███████▏  | 2666/3678 [34:40<13:09,  1.28it/s, dice_loss - 0.1817, iou_score - 0.7069]\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-8a96f05e38d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mvalid_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/utils/train.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m# update loss logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/utils/train.py\u001b[0m in \u001b[0;36mbatch_update\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = torch.load('first_model_1.pth')","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_epoch = smp.utils.train.ValidEpoch(\n    best_model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logs = test_epoch.run(valid_loader)","execution_count":33,"outputs":[{"output_type":"stream","text":"valid: 100%|██████████| 1634/1634 [01:59<00:00, 13.70it/s, dice_loss - 0.3283, iou_score - 0.577] \n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Create final submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_preprocessing(preprocessing_fn):\n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor),\n    ]\n    return albu.Compose(_transform)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pbar = tqdm(sample_sub.index[:])\nbest_model.eval()\nfor idx in pbar:  \n    fpath = os.path.join(test_image_dir, sample_sub.iloc[idx].ImageId)\n    c_img = imread(fpath)\n    processed_image = get_test_preprocessing(preprocessing_fn)(image=c_img)['image']\n    x_tensor = torch.from_numpy(processed_image).to(DEVICE).unsqueeze(0)\n    \n    mask = best_model.predict(x_tensor)\n    mask_ready = mask.cpu().squeeze(0).numpy()[0] > 0.5\n    encode_mask = rle_encode(mask_ready)\n    sample_sub.iloc[idx].EncodedPixels = encode_mask","execution_count":65,"outputs":[{"output_type":"stream","text":"100%|██████████| 2829/2829 [04:27<00:00, 10.57it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.to_csv('submission.csv', index=False)\nsample_sub.head()","execution_count":66,"outputs":[{"output_type":"execute_result","execution_count":66,"data":{"text/plain":"         ImageId                                      EncodedPixels\n0  0010551d9.jpg  183033 13 183799 22 184566 26 185333 29 186100...\n1  002a943bf.jpg  403196 3 403959 9 404724 12 405490 15 406256 1...\n2  0035268d9.jpg  455522 1 456290 2 457057 3 457825 3 458593 3 4...\n3  008f038d3.jpg  225018 5 225777 15 226540 21 227305 24 228067 ...\n4  009bc4be5.jpg  161347 5 162112 11 162878 16 163644 19 164411 ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>EncodedPixels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0010551d9.jpg</td>\n      <td>183033 13 183799 22 184566 26 185333 29 186100...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>002a943bf.jpg</td>\n      <td>403196 3 403959 9 404724 12 405490 15 406256 1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0035268d9.jpg</td>\n      <td>455522 1 456290 2 457057 3 457825 3 458593 3 4...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>008f038d3.jpg</td>\n      <td>225018 5 225777 15 226540 21 227305 24 228067 ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>009bc4be5.jpg</td>\n      <td>161347 5 162112 11 162878 16 163644 19 164411 ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sample_sub)","execution_count":67,"outputs":[{"output_type":"execute_result","execution_count":67,"data":{"text/plain":"2829"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}